{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ced648-5b1e-45e9-9bf6-2bc0fdac182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grocery] ['oil', 'cooking oil'] -> 421 items\n",
      "[Grocery] ['rice'] -> 678 items\n",
      "[Grocery] ['sugar'] -> 450 items\n",
      "[Grocery] ['pasta'] -> 375 items\n",
      "[Grocery] ['tea'] -> 1200 items\n",
      "[Grocery] ['coffee'] -> 1183 items\n",
      "[Grocery] ['milk'] -> 1200 items\n",
      "[Grocery] ['tuna'] -> 54 items\n",
      "[Grocery] ['tomato paste'] -> 4 items\n",
      "[Grocery] ['detergent'] -> 391 items\n",
      "[Grocery] ['dishwashing liquid'] -> 68 items\n",
      "[Grocery] ['diapers'] -> 919 items\n",
      "[Electronics] ['smartphone'] -> 1178 items\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from datetime import date\n",
    "\n",
    "# ================== إعدادات عامة ==================\n",
    "BASE = \"https://www.jumia.com.eg\"\n",
    "SEARCH_URL = \"https://www.jumia.com.eg/catalog/\"\n",
    "SOURCE_NAME = \"jumia\"\n",
    "RUN_DATE = date.today().isoformat()\n",
    "\n",
    "# فولدر واحد فقط: Data\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"Data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# ملف واحد يوميًا باسم prices_2025-12-27.csv\n",
    "OUTPUT_FILE = os.path.join(DATA_DIR, f\"prices_{RUN_DATE}.csv\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9,ar;q=0.8\",\n",
    "}\n",
    "\n",
    "SEARCH_MAP = {\n",
    "    \"Grocery\": [\n",
    "        [\"oil\", \"cooking oil\"],\n",
    "        [\"rice\"],\n",
    "        [\"sugar\"],\n",
    "        [\"pasta\"],\n",
    "        [\"tea\"],\n",
    "        [\"coffee\"],\n",
    "        [\"milk\"],\n",
    "        [\"tuna\"],\n",
    "        [\"tomato paste\"],\n",
    "        [\"detergent\"],\n",
    "        [\"dishwashing liquid\"],\n",
    "        [\"diapers\"]\n",
    "    ],\n",
    "    \"Electronics\": [\n",
    "        [\"smartphone\"],\n",
    "        [\"mobile phone\"],\n",
    "        [\"laptop\"],\n",
    "        [\"tablet\"],\n",
    "        [\"smart watch\"],\n",
    "        [\"earbuds\"],\n",
    "        [\"power bank\"],\n",
    "        [\"headphones\"]\n",
    "    ],\n",
    "    \"Home_Appliances\": [\n",
    "        [\"air fryer\"],\n",
    "        [\"microwave\"],\n",
    "        [\"coffee machine\"],\n",
    "        [\"electric kettle\"],\n",
    "        [\"blender\"],\n",
    "        [\"vacuum cleaner\"]\n",
    "    ],\n",
    "    \"TV_and_Screens\": [\n",
    "        [\"smart tv\"],\n",
    "        [\"tv\"],\n",
    "        [\"led tv\"],\n",
    "        [\"uhd tv\"],\n",
    "        [\"android tv\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ================== Helpers ==================\n",
    "def clean_text(x: str):\n",
    "    return \" \".join(x.split()).strip() if x else None\n",
    "\n",
    "def parse_price_egp(text: str):\n",
    "    if not text:\n",
    "        return None\n",
    "    t = text.replace(\"EGP\", \"\").replace(\",\", \"\").strip()\n",
    "    try:\n",
    "        return float(t)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fetch_html(query: str, page: int):\n",
    "    params = {\"q\": query}\n",
    "    if page > 1:\n",
    "        params[\"page\"] = page\n",
    "    r = requests.get(SEARCH_URL, params=params, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def parse_product_card(a_tag):\n",
    "    href = a_tag.get(\"href\")\n",
    "    product_url = urljoin(BASE, href) if href else None\n",
    "    name_el = a_tag.select_one(\"h3.name\")\n",
    "    product_name = clean_text(name_el.get_text()) if name_el else None\n",
    "    prc_el = a_tag.select_one(\"div.prc\")\n",
    "    price_text = clean_text(prc_el.get_text()) if prc_el else None\n",
    "    price = parse_price_egp(price_text)\n",
    "    old_el = a_tag.select_one(\"div.old\")\n",
    "    old_price_text = clean_text(old_el.get_text()) if old_el else None\n",
    "    old_price = parse_price_egp(old_price_text)\n",
    "    disc_el = a_tag.select_one(\"div.bdg._dsct\")\n",
    "    discount = clean_text(disc_el.get_text()) if disc_el else None\n",
    "    rating = None\n",
    "    reviews_count = None\n",
    "    rev = a_tag.select_one(\"div.rev\")\n",
    "    if rev:\n",
    "        stars = rev.select_one(\"div.stars\")\n",
    "        if stars:\n",
    "            rating = clean_text(stars.get_text()).replace(\"out of 5\", \"\").strip()\n",
    "        txt = clean_text(rev.get_text()) or \"\"\n",
    "        if \"(\" in txt and \")\" in txt:\n",
    "            try:\n",
    "                reviews_count = int(txt.split(\"(\")[-1].split(\")\")[0].strip())\n",
    "            except Exception:\n",
    "                reviews_count = None\n",
    "    img_url = None\n",
    "    img = a_tag.select_one(\"img.img\")\n",
    "    if img:\n",
    "        img_url = img.get(\"data-src\") or img.get(\"src\")\n",
    "    brand = a_tag.get(\"data-gtm-brand\") or a_tag.get(\"data-ga4-item_brand\")\n",
    "    item_id = a_tag.get(\"data-gtm-id\") or a_tag.get(\"data-ga4-item_id\")\n",
    "    category_path = a_tag.get(\"data-gtm-category\") or a_tag.get(\"data-ga4-item_category\")\n",
    "    return {\n",
    "        \"date\": RUN_DATE,\n",
    "        \"source\": SOURCE_NAME,\n",
    "        \"category\": None,\n",
    "        \"search_query\": None,\n",
    "        \"page\": None,\n",
    "        \"item_id\": item_id,\n",
    "        \"brand\": brand,\n",
    "        \"category_path\": category_path,\n",
    "        \"product_name\": product_name,\n",
    "        \"price\": price,\n",
    "        \"old_price\": old_price,\n",
    "        \"discount\": discount,\n",
    "        \"rating\": rating,\n",
    "        \"reviews_count\": reviews_count,\n",
    "        \"image_url\": img_url,\n",
    "        \"product_url\": product_url,\n",
    "    }\n",
    "\n",
    "def scrape_one_keyword(keyword: str, category: str, max_pages: int = 30, sleep_range=(1.5, 3.0)):\n",
    "    rows = []\n",
    "    seen = set()\n",
    "    for page in range(1, max_pages + 1):\n",
    "        try:\n",
    "            html = fetch_html(keyword, page)\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            cards = soup.select(\"a.core[href*='.html']\")\n",
    "            if not cards:\n",
    "                break\n",
    "            new_count = 0\n",
    "            for a in cards:\n",
    "                data = parse_product_card(a)\n",
    "                key = data[\"item_id\"] or data[\"product_url\"]\n",
    "                if not key or key in seen:\n",
    "                    continue\n",
    "                seen.add(key)\n",
    "                data[\"category\"] = category\n",
    "                data[\"search_query\"] = keyword\n",
    "                data[\"page\"] = page\n",
    "                rows.append(data)\n",
    "                new_count += 1\n",
    "            if new_count == 0:\n",
    "                break\n",
    "            time.sleep(random.uniform(*sleep_range))\n",
    "        except Exception as e:\n",
    "            print(f\"خطأ في صفحة {page} للكلمة '{keyword}': {e}\")\n",
    "            break  # لو حصل خطأ في صفحة، نوقف الكلمة دي ونكمل الكلمات التانية\n",
    "    return rows\n",
    "\n",
    "def save_daily_file(path: str, rows: list):\n",
    "    if not rows:\n",
    "        print(\"No data collected.\")\n",
    "        return\n",
    "    \n",
    "    fieldnames = [\n",
    "        \"date\", \"source\", \"category\", \"search_query\", \"page\",\n",
    "        \"item_id\", \"brand\", \"category_path\",\n",
    "        \"product_name\", \"price\", \"old_price\", \"discount\",\n",
    "        \"rating\", \"reviews_count\",\n",
    "        \"image_url\", \"product_url\"\n",
    "    ]\n",
    "    \n",
    "    file_exists = os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    print(f\"تم حفظ/إضافة {len(rows)} منتج في: {path}\")\n",
    "\n",
    "def run_all(max_pages_per_query: int = 30):\n",
    "    all_rows = []\n",
    "    try:\n",
    "        for category, keywords in SEARCH_MAP.items():\n",
    "            for kw in keywords:\n",
    "                print(f\"جاري جمع [{category}] → '{kw}' ...\")\n",
    "                rows = scrape_one_keyword(kw, category, max_pages=max_pages_per_query)\n",
    "                all_rows.extend(rows)\n",
    "                print(f\"    → تم جمع {len(rows)} منتج\")\n",
    "                # نحفظ كل شوية عشان لو حصل خطأ في النص نكون محتفظين باللي اتجمع\n",
    "                if rows:\n",
    "                    save_daily_file(OUTPUT_FILE, rows)\n",
    "                    rows.clear()  # نفرغ القايمة عشان ما نكررش الحفظ\n",
    "\n",
    "        # حفظ نهائي لأي بيانات متبقية\n",
    "        if all_rows:\n",
    "            save_daily_file(OUTPUT_FILE, all_rows)\n",
    "\n",
    "        print(f\"\\nتم الانتهاء بنجاح! الملف: {OUTPUT_FILE}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nحصل خطأ كبير: {e}\")\n",
    "        print(\"لكن تم حفظ كل اللي اتجمع لحد دلوقتي!\")\n",
    "        if all_rows:\n",
    "            save_daily_file(OUTPUT_FILE, all_rows)\n",
    "    finally:\n",
    "        # ضمان الحفظ دايماً في النهاية\n",
    "        if all_rows:\n",
    "            save_daily_file(OUTPUT_FILE, all_rows)\n",
    "        print(f\"الملف النهائي: {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all(max_pages_per_query=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a4b7c-85c2-45e4-aa59-b2bf56ec5021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2f633-1347-4e09-97c7-57c451165616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
