{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8a93a8-c869-48c2-b0c1-1224341f8050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install undetected-chromedriver selenium beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af2942-58aa-4e92-ba9b-eda3b6a4583a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c71e6-6990-4529-b12a-cccc49911e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a82df2-6c37-4eff-9dcc-ff4446df02bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "from datetime import date\n",
    "from urllib.parse import quote_plus, urljoin\n",
    "\n",
    "import undetected_chromedriver as uc\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "BASE = \"https://www.amazon.eg\"\n",
    "\n",
    "SCRAPE_DATE = date.today().isoformat()  # YYYY-MM-DD\n",
    "\n",
    "DATA_DIR = \"Data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "OUT_CSV = os.path.join(DATA_DIR, f\"amazon_eg_{SCRAPE_DATE}.csv\")\n",
    "\n",
    "MAX_PAGES_PER_QUERY = 7  # cap pages per query\n",
    "\n",
    "\n",
    "# ===================== SEARCH MAP (groups) =====================\n",
    "# Each inner list is a \"keyword group\" -> we join it into one query string.\n",
    "# Example: [\"oil\", \"cooking oil\"] -> \"oil cooking oil\"\n",
    "SEARCH_MAP = {\n",
    "    \"Grocery\": [\n",
    "        [\"oil\", \"cooking oil\"],\n",
    "        [\"rice\"],\n",
    "        [\"sugar\"],\n",
    "        [\"pasta\"],\n",
    "        [\"tea\"],\n",
    "        [\"coffee\"],\n",
    "        [\"milk\"],\n",
    "        [\"tuna\"],\n",
    "        [\"tomato paste\"],\n",
    "        [\"detergent\"],\n",
    "        [\"dishwashing liquid\"],\n",
    "        [\"diapers\"]\n",
    "    ],\n",
    "\n",
    "    \"Electronics\": [\n",
    "        [\"smartphone\"],\n",
    "        [\"mobile phone\"],\n",
    "        [\"laptop\"],\n",
    "        [\"tablet\"],\n",
    "        [\"smart watch\"],\n",
    "        [\"earbuds\"],\n",
    "        [\"power bank\"],\n",
    "        [\"headphones\"]\n",
    "    ],\n",
    "\n",
    "    \"Home_Appliances\": [\n",
    "        [\"air fryer\"],\n",
    "        [\"microwave\"],\n",
    "        [\"coffee machine\"],\n",
    "        [\"electric kettle\"],\n",
    "        [\"blender\"],\n",
    "        [\"vacuum cleaner\"]\n",
    "    ],\n",
    "\n",
    "    \"TV_and_Screens\": [\n",
    "        [\"smart tv\"],\n",
    "        [\"tv\"],\n",
    "        [\"led tv\"],\n",
    "        [\"uhd tv\"],\n",
    "        [\"android tv\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# ===================== HELPERS =====================\n",
    "def clean_text(x: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (x or \"\")).strip()\n",
    "\n",
    "\n",
    "def is_blocked(html: str) -> bool:\n",
    "    return bool(re.search(\n",
    "        r\"Robot Check|Enter the characters you see|Sorry, we just need to make sure|captcha\",\n",
    "        html,\n",
    "        re.I\n",
    "    ))\n",
    "\n",
    "\n",
    "def build_driver(headless: bool = False):\n",
    "    options = uc.ChromeOptions()\n",
    "\n",
    "    # headless may increase blocking; start with False\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "\n",
    "    options.add_argument(\"--window-size=1280,900\")\n",
    "    options.add_argument(\"--lang=en-US\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    options.add_argument(\n",
    "        \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    driver = uc.Chrome(options=options)\n",
    "    driver.set_page_load_timeout(60)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def human_scroll(driver):\n",
    "    steps = random.randint(2, 4)\n",
    "    for i in range(steps):\n",
    "        driver.execute_script(\n",
    "            f\"window.scrollTo(0, document.body.scrollHeight*{(i + 1) / (steps + 1)});\"\n",
    "        )\n",
    "        time.sleep(random.uniform(1.0, 2.0))\n",
    "\n",
    "\n",
    "def fetch_html(driver, url: str) -> str:\n",
    "    driver.get(url)\n",
    "    time.sleep(random.uniform(3.5, 5.5))\n",
    "    human_scroll(driver)\n",
    "    time.sleep(random.uniform(1.5, 2.5))\n",
    "    return driver.page_source\n",
    "\n",
    "\n",
    "def get_search_url(query: str, page: int) -> str:\n",
    "    return f\"{BASE}/s?k={quote_plus(query)}&page={page}\"\n",
    "\n",
    "\n",
    "# ===================== EXTRACTORS =====================\n",
    "def extract_title_and_url(card):\n",
    "    \"\"\"\n",
    "    Title priority:\n",
    "      1) h2[aria-label] (most accurate)\n",
    "      2) h2 span\n",
    "    URL:\n",
    "      1) parent anchor of h2\n",
    "      2) fallback any /dp/ link\n",
    "    \"\"\"\n",
    "    title, url = \"\", \"\"\n",
    "\n",
    "    h2 = card.select_one(\"h2.a-size-base-plus\")\n",
    "    if h2:\n",
    "        if h2.get(\"aria-label\"):\n",
    "            title = clean_text(h2.get(\"aria-label\"))\n",
    "\n",
    "        if not title:\n",
    "            span = h2.select_one(\"span\")\n",
    "            if span:\n",
    "                title = clean_text(span.get_text())\n",
    "\n",
    "        a = h2.find_parent(\"a\", href=True)\n",
    "        if a and a.get(\"href\"):\n",
    "            url = urljoin(BASE, a.get(\"href\"))\n",
    "\n",
    "    if not url:\n",
    "        a2 = card.select_one('a[href*=\"/dp/\"]')\n",
    "        if a2 and a2.get(\"href\"):\n",
    "            url = urljoin(BASE, a2.get(\"href\"))\n",
    "\n",
    "    return title, url\n",
    "\n",
    "\n",
    "def extract_price(card) -> str:\n",
    "    price_el = card.select_one(\"span.a-price > span.a-offscreen\")\n",
    "    return clean_text(price_el.get_text()) if price_el else \"\"\n",
    "\n",
    "\n",
    "def extract_old_price(card) -> str:\n",
    "    # Old price: List: or Was:\n",
    "    for prefix in (\"List:\", \"Was:\"):\n",
    "        block = card.select_one(\n",
    "            f'div[aria-hidden^=\"{prefix}\"] span.a-price.a-text-price span.a-offscreen'\n",
    "        )\n",
    "        if block and clean_text(block.get_text()):\n",
    "            return clean_text(block.get_text())\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_rating_reviews(card):\n",
    "    rating_el = card.select_one(\"span.a-icon-alt\")  # e.g. \"4.1 out of 5 stars\"\n",
    "    rating = clean_text(rating_el.get_text()) if rating_el else \"\"\n",
    "\n",
    "    reviews_el = card.select_one('a[href*=\"#customerReviews\"] span') or card.select_one(\"span.a-size-base\")\n",
    "    reviews = clean_text(reviews_el.get_text()) if reviews_el else \"\"\n",
    "\n",
    "    return rating, reviews\n",
    "\n",
    "\n",
    "def extract_stock_status(card):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      stock_status: in_stock / out_of_stock / unknown\n",
    "      stock_text: message if found\n",
    "    \"\"\"\n",
    "    aria = card.select_one('span[aria-label*=\"stock\" i], span[aria-label*=\"unavailable\" i]')\n",
    "    msg = clean_text(aria.get(\"aria-label\")) if aria and aria.get(\"aria-label\") else \"\"\n",
    "\n",
    "    if msg:\n",
    "        low = msg.lower()\n",
    "        if \"left in stock\" in low or \"in stock\" in low:\n",
    "            return \"in_stock\", msg\n",
    "        if \"currently unavailable\" in low or \"out of stock\" in low or \"unavailable\" in low:\n",
    "            return \"out_of_stock\", msg\n",
    "\n",
    "    txt = card.get_text(\" \", strip=True).lower()\n",
    "    if \"currently unavailable\" in txt or \"out of stock\" in txt:\n",
    "        return \"out_of_stock\", \"Currently unavailable / Out of stock\"\n",
    "    if \"left in stock\" in txt or \"in stock\" in txt:\n",
    "        return \"in_stock\", \"In stock\"\n",
    "\n",
    "    return \"unknown\", \"\"\n",
    "\n",
    "\n",
    "# ===================== PARSING =====================\n",
    "def parse_search_page(html: str, category: str, group_keywords: list[str], query: str):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    rows = []\n",
    "\n",
    "    for card in soup.select('div[data-component-type=\"s-search-result\"][data-asin]'):\n",
    "        asin = (card.get(\"data-asin\") or \"\").strip()\n",
    "        if not asin:\n",
    "            continue\n",
    "\n",
    "        title, url = extract_title_and_url(card)\n",
    "        if not title and not url:\n",
    "            continue\n",
    "\n",
    "        price = extract_price(card)\n",
    "        old_price = extract_old_price(card)\n",
    "        rating, reviews = extract_rating_reviews(card)\n",
    "        stock_status, stock_text = extract_stock_status(card)\n",
    "\n",
    "        rows.append({\n",
    "            \"scrape_date\": SCRAPE_DATE,\n",
    "            \"category\": category,\n",
    "            \"keyword_group\": \" | \".join(group_keywords),  # keep original group\n",
    "            \"query\": query,  # joined group query used in URL\n",
    "            \"asin\": asin,\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"price\": price,\n",
    "            \"old_price\": old_price,\n",
    "            \"rating\": rating,\n",
    "            \"reviews\": reviews,\n",
    "            \"stock_status\": stock_status,\n",
    "            \"stock_text\": stock_text\n",
    "        })\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ===================== PAGE COUNT =====================\n",
    "def extract_total_results(html: str) -> int:\n",
    "    \"\"\"\n",
    "    Tries to read: \"1-48 of over 10,000 results for\" OR \"1-48 of 312 results for\"\n",
    "    If not found, returns 0.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "\n",
    "    m = re.search(r\"of\\s+(over\\s+)?([\\d,]+)\\s+results\", text, re.I)\n",
    "    if not m:\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        return int(m.group(2).replace(\",\", \"\"))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def compute_pages_to_scrape(first_page_html: str, max_pages: int) -> int:\n",
    "    total_results = extract_total_results(first_page_html)\n",
    "    if total_results <= 0:\n",
    "        return 0\n",
    "\n",
    "    # Commonly ~48 items per page on desktop (may vary)\n",
    "    estimated_pages = max(1, math.ceil(total_results / 48))\n",
    "    return min(estimated_pages, max_pages)\n",
    "\n",
    "\n",
    "# ===================== MAIN =====================\n",
    "def scrape_daily(search_map: dict, max_pages_per_query: int = MAX_PAGES_PER_QUERY, out_csv: str = OUT_CSV, headless: bool = False):\n",
    "    driver = build_driver(headless=headless)\n",
    "    all_rows = []\n",
    "\n",
    "    try:\n",
    "        # warm session\n",
    "        driver.get(BASE)\n",
    "        time.sleep(random.uniform(2.0, 4.0))\n",
    "\n",
    "        for category, keyword_groups in search_map.items():\n",
    "            for group in keyword_groups:\n",
    "                # group is list[str]\n",
    "                group_keywords = group\n",
    "                query = \" \".join(group_keywords).strip()\n",
    "\n",
    "                if not query:\n",
    "                    continue\n",
    "\n",
    "                # ---- page 1 first ----\n",
    "                url1 = get_search_url(query, 1)\n",
    "                html1 = fetch_html(driver, url1)\n",
    "\n",
    "                if is_blocked(html1):\n",
    "                    raise RuntimeError(\n",
    "                        \"CAPTCHA / Robot Check detected. Run headless=False, solve it once, then rerun.\"\n",
    "                    )\n",
    "\n",
    "                pages_to_scrape = compute_pages_to_scrape(html1, max_pages_per_query)\n",
    "                if pages_to_scrape == 0:\n",
    "                    print(f\"[{SCRAPE_DATE}] SKIP (no results): {category} | {group_keywords}\")\n",
    "                    continue\n",
    "\n",
    "                rows1 = parse_search_page(html1, category, group_keywords, query)\n",
    "                print(f\"[{SCRAPE_DATE}] {category} | {group_keywords} | page 1/{pages_to_scrape} -> {len(rows1)} items\")\n",
    "                all_rows.extend(rows1)\n",
    "\n",
    "                # early stop if page1 has no cards\n",
    "                if len(rows1) == 0:\n",
    "                    continue\n",
    "\n",
    "                time.sleep(random.uniform(3.0, 6.0))\n",
    "\n",
    "                # ---- remaining pages ----\n",
    "                for p in range(2, pages_to_scrape + 1):\n",
    "                    urlp = get_search_url(query, p)\n",
    "                    htmlp = fetch_html(driver, urlp)\n",
    "\n",
    "                    if is_blocked(htmlp):\n",
    "                        raise RuntimeError(\n",
    "                            \"CAPTCHA / Robot Check detected during paging. Reduce pages/delays or run headless=False.\"\n",
    "                        )\n",
    "\n",
    "                    rowsp = parse_search_page(htmlp, category, group_keywords, query)\n",
    "                    print(f\"[{SCRAPE_DATE}] {category} | {group_keywords} | page {p}/{pages_to_scrape} -> {len(rowsp)} items\")\n",
    "                    all_rows.extend(rowsp)\n",
    "\n",
    "                    # stop early if empty\n",
    "                    if len(rowsp) == 0:\n",
    "                        break\n",
    "\n",
    "                    time.sleep(random.uniform(3.0, 6.0))\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    # save CSV\n",
    "    if all_rows:\n",
    "        with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=list(all_rows[0].keys()))\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_rows)\n",
    "\n",
    "    return all_rows\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_daily(\n",
    "        search_map=SEARCH_MAP,\n",
    "        max_pages_per_query=MAX_PAGES_PER_QUERY,\n",
    "        out_csv=OUT_CSV,\n",
    "        headless=False  # start with False\n",
    "    )\n",
    "\n",
    "    print(\"DONE:\", len(data), \"->\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c5f253-c57a-432b-a04f-20e668507fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca80eea-704a-40e2-aab8-a8f350404129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5367596-d466-405b-8f85-b86258001256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5223fb9-c7b7-4107-b8ec-9299a12df0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d36abe-0d76-4862-9e55-7444589185a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f760f863-f5a6-434a-8f7b-0754ac1d5cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9953fc0-75cc-44d3-9144-cbb9c39e7ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-25] Category='Mobiles' Query='iPhone 16' Page=1/4 -> 51 items\n",
      "[2025-12-25] Category='Mobiles' Query='iPhone 16' Page=2/4 -> 58 items\n",
      "[2025-12-25] Category='Mobiles' Query='iPhone 16' Page=3/4 -> 60 items\n",
      "[2025-12-25] Category='Mobiles' Query='iPhone 16' Page=4/4 -> 60 items\n",
      "[2025-12-25] Category='Mobiles' Query='iPhone 15' Page=1/4 -> 48 items\n",
      "[2025-12-25] Category='Mobiles' Query='iPhone 15' Page=2/4 -> 52 items\n",
      "[2025-12-25] Category='Mobiles' Query='iPhone 15' Page=3/4 -> 52 items\n",
      "[2025-12-25] Category='Mobiles' Query='iPhone 15' Page=4/4 -> 56 items\n",
      "[2025-12-25] Category='Mobiles' Query='iPhone 14' Page=1/4 -> 48 items\n",
      "[2025-12-25] Category='Mobiles' Query='iPhone 14' Page=2/4 -> 55 items\n",
      "[2025-12-25] Category='Mobiles' Query='iPhone 14' Page=3/4 -> 60 items\n",
      "[2025-12-25] Category='Mobiles' Query='iPhone 14' Page=4/4 -> 60 items\n",
      "[2025-12-25] Category='Mobiles' Query='Samsung Galaxy S' Page=1/4 -> 50 items\n",
      "[2025-12-25] Category='Mobiles' Query='Samsung Galaxy S' Page=2/4 -> 50 items\n",
      "[2025-12-25] Category='Mobiles' Query='Samsung Galaxy S' Page=3/4 -> 51 items\n",
      "[2025-12-25] Category='Mobiles' Query='Samsung Galaxy S' Page=4/4 -> 53 items\n",
      "[2025-12-25] Category='Mobiles' Query='Samsung Galaxy A' Page=1/5 -> 48 items\n",
      "[2025-12-25] Category='Mobiles' Query='Samsung Galaxy A' Page=2/5 -> 51 items\n",
      "[2025-12-25] Category='Mobiles' Query='Samsung Galaxy A' Page=3/5 -> 51 items\n",
      "[2025-12-25] Category='Mobiles' Query='Samsung Galaxy A' Page=4/5 -> 51 items\n",
      "[2025-12-25] Category='Mobiles' Query='Samsung Galaxy A' Page=5/5 -> 48 items\n",
      "[2025-12-25] Category='Mobiles' Query='Xiaomi Redmi Note' Page=1/5 -> 48 items\n",
      "[2025-12-25] Category='Mobiles' Query='Xiaomi Redmi Note' Page=2/5 -> 56 items\n",
      "[2025-12-25] Category='Mobiles' Query='Xiaomi Redmi Note' Page=3/5 -> 54 items\n",
      "[2025-12-25] Category='Mobiles' Query='Xiaomi Redmi Note' Page=4/5 -> 57 items\n",
      "[2025-12-25] Category='Mobiles' Query='Xiaomi Redmi Note' Page=5/5 -> 55 items\n",
      "[2025-12-25] Category='Mobiles' Query='realme' Page=1/5 -> 49 items\n",
      "[2025-12-25] Category='Mobiles' Query='realme' Page=2/5 -> 49 items\n",
      "[2025-12-25] Category='Mobiles' Query='realme' Page=3/5 -> 49 items\n",
      "[2025-12-25] Category='Mobiles' Query='realme' Page=4/5 -> 49 items\n",
      "[2025-12-25] Category='Mobiles' Query='realme' Page=5/5 -> 49 items\n",
      "[2025-12-25] Category='Mobiles' Query='realme c55' Page=1/5 -> 48 items\n",
      "[2025-12-25] Category='Mobiles' Query='realme c55' Page=2/5 -> 52 items\n",
      "[2025-12-25] Category='Mobiles' Query='realme c55' Page=3/5 -> 52 items\n",
      "[2025-12-25] Category='Mobiles' Query='realme c55' Page=4/5 -> 54 items\n",
      "[2025-12-25] Category='Mobiles' Query='realme c55' Page=5/5 -> 56 items\n",
      "[2025-12-25] Category='Laptops' Query='MacBook Air' Page=1/5 -> 56 items\n",
      "[2025-12-25] Category='Laptops' Query='MacBook Air' Page=2/5 -> 56 items\n",
      "[2025-12-25] Category='Laptops' Query='MacBook Air' Page=3/5 -> 56 items\n",
      "[2025-12-25] Category='Laptops' Query='MacBook Air' Page=4/5 -> 56 items\n",
      "[2025-12-25] Category='Laptops' Query='MacBook Air' Page=5/5 -> 56 items\n",
      "[2025-12-25] Category='Laptops' Query='HP Pavilion' Page=1/4 -> 48 items\n",
      "[2025-12-25] Category='Laptops' Query='HP Pavilion' Page=2/4 -> 52 items\n",
      "[2025-12-25] Category='Laptops' Query='HP Pavilion' Page=3/4 -> 52 items\n",
      "[2025-12-25] Category='Laptops' Query='HP Pavilion' Page=4/4 -> 52 items\n",
      "[2025-12-25] Category='Laptops' Query='HP Victus' Page=1/2 -> 50 items\n",
      "[2025-12-25] Category='Laptops' Query='HP Victus' Page=2/2 -> 24 items\n",
      "[2025-12-25] Category='Laptops' Query='Dell Vostro' Page=1/4 -> 48 items\n",
      "[2025-12-25] Category='Laptops' Query='Dell Vostro' Page=2/4 -> 52 items\n",
      "[2025-12-25] Category='Laptops' Query='Dell Vostro' Page=3/4 -> 52 items\n",
      "[2025-12-25] Category='Laptops' Query='Dell Vostro' Page=4/4 -> 17 items\n",
      "[2025-12-25] Category='Laptops' Query='Dell Inspiron' Page=1/5 -> 49 items\n",
      "[2025-12-25] Category='Laptops' Query='Dell Inspiron' Page=2/5 -> 54 items\n",
      "[2025-12-25] Category='Laptops' Query='Dell Inspiron' Page=3/5 -> 56 items\n",
      "[2025-12-25] Category='Laptops' Query='Dell Inspiron' Page=4/5 -> 56 items\n",
      "[2025-12-25] Category='Laptops' Query='Dell Inspiron' Page=5/5 -> 56 items\n",
      "[2025-12-25] Category='Laptops' Query='Lenovo IdeaPad' Page=1/4 -> 51 items\n",
      "[2025-12-25] Category='Laptops' Query='Lenovo IdeaPad' Page=2/4 -> 54 items\n",
      "[2025-12-25] Category='Laptops' Query='Lenovo IdeaPad' Page=3/4 -> 52 items\n",
      "[2025-12-25] Category='Laptops' Query='Lenovo IdeaPad' Page=4/4 -> 51 items\n",
      "[2025-12-25] Category='Laptops' Query='ASUS VivoBook 15' Page=1/3 -> 50 items\n",
      "[2025-12-25] Category='Laptops' Query='ASUS VivoBook 15' Page=2/3 -> 52 items\n",
      "[2025-12-25] Category='Laptops' Query='ASUS VivoBook 15' Page=3/3 -> 9 items\n",
      "[2025-12-25] Category='Audio' Query='AirPods Pro' Page=1/5 -> 50 items\n",
      "[2025-12-25] Category='Audio' Query='AirPods Pro' Page=2/5 -> 56 items\n",
      "[2025-12-25] Category='Audio' Query='AirPods Pro' Page=3/5 -> 56 items\n",
      "[2025-12-25] Category='Audio' Query='AirPods Pro' Page=4/5 -> 56 items\n",
      "[2025-12-25] Category='Audio' Query='AirPods Pro' Page=5/5 -> 56 items\n",
      "[2025-12-25] Category='Audio' Query='AirPods' Page=1/5 -> 52 items\n",
      "[2025-12-25] Category='Audio' Query='AirPods' Page=2/5 -> 56 items\n",
      "[2025-12-25] Category='Audio' Query='AirPods' Page=3/5 -> 56 items\n",
      "[2025-12-25] Category='Audio' Query='AirPods' Page=4/5 -> 56 items\n",
      "[2025-12-25] Category='Audio' Query='AirPods' Page=5/5 -> 56 items\n",
      "[2025-12-25] Category='Audio' Query='Samsung Galaxy Buds2 Pro' Page=1/2 -> 48 items\n",
      "[2025-12-25] Category='Audio' Query='Samsung Galaxy Buds2 Pro' Page=2/2 -> 19 items\n",
      "[2025-12-25] Category='Audio' Query='Samsung Buds FE' Page=1/3 -> 48 items\n",
      "[2025-12-25] Category='Audio' Query='Samsung Buds FE' Page=2/3 -> 50 items\n",
      "[2025-12-25] Category='Audio' Query='Samsung Buds FE' Page=3/3 -> 7 items\n",
      "[2025-12-25] Category='Audio' Query='JBL Tune 510BT' Page=1/2 -> 48 items\n",
      "[2025-12-25] Category='Audio' Query='JBL Tune 510BT' Page=2/2 -> 11 items\n",
      "[2025-12-25] Category='Wearables' Query='Apple Watch Ultra' Page=1/5 -> 49 items\n",
      "[2025-12-25] Category='Wearables' Query='Apple Watch Ultra' Page=2/5 -> 56 items\n",
      "[2025-12-25] Category='Wearables' Query='Apple Watch Ultra' Page=3/5 -> 60 items\n",
      "[2025-12-25] Category='Wearables' Query='Apple Watch Ultra' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Wearables' Query='Apple Watch Ultra' Page=5/5 -> 60 items\n",
      "[2025-12-25] Category='Wearables' Query='Apple Watch Series' Page=1/5 -> 49 items\n",
      "[2025-12-25] Category='Wearables' Query='Apple Watch Series' Page=2/5 -> 53 items\n",
      "[2025-12-25] Category='Wearables' Query='Apple Watch Series' Page=3/5 -> 56 items\n",
      "[2025-12-25] Category='Wearables' Query='Apple Watch Series' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Wearables' Query='Apple Watch Series' Page=5/5 -> 60 items\n",
      "[2025-12-25] Category='Wearables' Query='Samsung Galaxy Watch' Page=1/5 -> 53 items\n",
      "[2025-12-25] Category='Wearables' Query='Samsung Galaxy Watch' Page=2/5 -> 58 items\n",
      "[2025-12-25] Category='Wearables' Query='Samsung Galaxy Watch' Page=3/5 -> 59 items\n",
      "[2025-12-25] Category='Wearables' Query='Samsung Galaxy Watch' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Wearables' Query='Samsung Galaxy Watch' Page=5/5 -> 59 items\n",
      "[2025-12-25] Category='Wearables' Query='Xiaomi Smart Band' Page=1/5 -> 49 items\n",
      "[2025-12-25] Category='Wearables' Query='Xiaomi Smart Band' Page=2/5 -> 56 items\n",
      "[2025-12-25] Category='Wearables' Query='Xiaomi Smart Band' Page=3/5 -> 57 items\n",
      "[2025-12-25] Category='Wearables' Query='Xiaomi Smart Band' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Wearables' Query='Xiaomi Smart Band' Page=5/5 -> 57 items\n",
      "[2025-12-25] Category='Accessories' Query='USB C charger' Page=1/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='USB C charger' Page=2/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='USB C charger' Page=3/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='USB C charger' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='USB C charger' Page=5/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='Anker power bank' Page=1/3 -> 56 items\n",
      "[2025-12-25] Category='Accessories' Query='Anker power bank' Page=2/3 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='Anker power bank' Page=3/3 -> 24 items\n",
      "[2025-12-25] Category='Accessories' Query='power bank 20000mAh' Page=1/2 -> 52 items\n",
      "[2025-12-25] Category='Accessories' Query='power bank 20000mAh' Page=2/2 -> 54 items\n",
      "[2025-12-25] Category='Accessories' Query='iphone case' Page=1/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='iphone case' Page=2/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='iphone case' Page=3/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='iphone case' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='iphone case' Page=5/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='samsung a55 case' Page=1/4 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='samsung a55 case' Page=2/4 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='samsung a55 case' Page=3/4 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='samsung a55 case' Page=4/4 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='USB C cable' Page=1/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='USB C cable' Page=2/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='USB C cable' Page=3/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='USB C cable' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='USB C cable' Page=5/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='car charger USB C' Page=1/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='car charger USB C' Page=2/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='car charger USB C' Page=3/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='car charger USB C' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Accessories' Query='car charger USB C' Page=5/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='air fryer' Page=1/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='air fryer' Page=2/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='air fryer' Page=3/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='air fryer' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='air fryer' Page=5/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='espresso machine' Page=1/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='espresso machine' Page=2/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='espresso machine' Page=3/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='espresso machine' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='espresso machine' Page=5/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='microwave' Page=1/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='microwave' Page=2/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='microwave' Page=3/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='microwave' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='microwave' Page=5/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='electric kettle' Page=1/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='electric kettle' Page=2/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='electric kettle' Page=3/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='electric kettle' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Appliances' Query='electric kettle' Page=5/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='sunflower oil' Page=1/2 -> 48 items\n",
      "[2025-12-25] Category='Grocery' Query='sunflower oil' Page=2/2 -> 42 items\n",
      "[2025-12-25] Category='Grocery' Query='corn oil' Page=1/2 -> 48 items\n",
      "[2025-12-25] Category='Grocery' Query='corn oil' Page=2/2 -> 9 items\n",
      "[2025-12-25] Category='Grocery' Query='white rice' Page=1/2 -> 56 items\n",
      "[2025-12-25] Category='Grocery' Query='white rice' Page=2/2 -> 48 items\n",
      "[2025-12-25] Category='Grocery' Query='pasta' Page=1/5 -> 57 items\n",
      "[2025-12-25] Category='Grocery' Query='pasta' Page=2/5 -> 57 items\n",
      "[2025-12-25] Category='Grocery' Query='pasta' Page=3/5 -> 57 items\n",
      "[2025-12-25] Category='Grocery' Query='pasta' Page=4/5 -> 57 items\n",
      "[2025-12-25] Category='Grocery' Query='pasta' Page=5/5 -> 57 items\n",
      "[2025-12-25] Category='Grocery' Query='sugar' Page=1/3 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='sugar' Page=2/3 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='sugar' Page=3/3 -> 58 items\n",
      "[2025-12-25] Category='Grocery' Query='ghee' Page=1/2 -> 48 items\n",
      "[2025-12-25] Category='Grocery' Query='ghee' Page=2/2 -> 37 items\n",
      "[2025-12-25] Category='Grocery' Query='full cream milk' Page=1/2 -> 49 items\n",
      "[2025-12-25] Category='Grocery' Query='full cream milk' Page=2/2 -> 27 items\n",
      "[2025-12-25] Category='Grocery' Query='instant coffee' Page=1/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='instant coffee' Page=2/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='instant coffee' Page=3/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='instant coffee' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='instant coffee' Page=5/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='tea bags' Page=1/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='tea bags' Page=2/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='tea bags' Page=3/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='tea bags' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='tea bags' Page=5/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='cheddar cheese' Page=1/2 -> 48 items\n",
      "[2025-12-25] Category='Grocery' Query='cheddar cheese' Page=2/2 -> 34 items\n",
      "[2025-12-25] Category='Grocery' Query='laundry detergent powder' Page=1/4 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='laundry detergent powder' Page=2/4 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='laundry detergent powder' Page=3/4 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='laundry detergent powder' Page=4/4 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='dishwashing liquid' Page=1/4 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='dishwashing liquid' Page=2/4 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='dishwashing liquid' Page=3/4 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='dishwashing liquid' Page=4/4 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='shampoo' Page=1/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='shampoo' Page=2/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='shampoo' Page=3/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='shampoo' Page=4/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='shampoo' Page=5/5 -> 60 items\n",
      "[2025-12-25] Category='Grocery' Query='toilet paper roll' Page=1/4 -> 53 items\n",
      "[2025-12-25] Category='Grocery' Query='toilet paper roll' Page=2/4 -> 53 items\n",
      "[2025-12-25] Category='Grocery' Query='toilet paper roll' Page=3/4 -> 53 items\n",
      "[2025-12-25] Category='Grocery' Query='toilet paper roll' Page=4/4 -> 53 items\n",
      "DONE: 10554 -> amazon_eg_2025-12-25.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import csv\n",
    "from datetime import date\n",
    "from urllib.parse import quote_plus, urljoin\n",
    "\n",
    "import undetected_chromedriver as uc\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "BASE = \"https://www.amazon.eg\"\n",
    "\n",
    "# ===================== إعدادات التجميع اليومي =====================\n",
    "SCRAPE_DATE = date.today().isoformat()\n",
    "OUT_CSV = f\"amazon_eg_{SCRAPE_DATE}.csv\"\n",
    "\n",
    "# ===================== Max pages =====================\n",
    "MAX_PAGES_PER_QUERY = 5  # ✅ لو أكتر من كده نجمع 5 بس\n",
    "\n",
    "SEARCH_MAP = {\n",
    "    # الفئة دي هي الأسهل في الـ Matching لأن الموديلات واضحة جداً\n",
    "    \"Smartphones\": [\n",
    "        \"iPhone 16 128GB\", \"iPhone 15 128GB\", \"Samsung Galaxy S24 Ultra\", \n",
    "        \"Samsung Galaxy A55\", \"Xiaomi Redmi Note 13\", \"realme 12 Pro\"\n",
    "    ],\n",
    "\n",
    "    # فئة أساسية لكشف خصومات \"الجمعة البيضاء\" الوهمية\n",
    "    \"Laptops_and_Screens\": [\n",
    "        \"MacBook Air M2 13\", \"HP Victus 15\", \"Lenovo IdeaPad 3\",\n",
    "        \"Samsung TV 43 inch Crystal\", \"LG TV 50 inch UHD\", \"Toshiba TV 43 inch\"\n",
    "    ],\n",
    "\n",
    "    # فئة التضخم (Inflation) - ركزت على براندات محددة لضمان دقة المقارنة\n",
    "    \"Grocery_Inflation_Index\": [\n",
    "        \"Crystal sunflower oil 0.8L\", \"Sultana Rice 1kg\", \"Lipton Tea 100 Bags\",\n",
    "        \"Nescafe Red Cup 200g\", \"Almarai Milk Full Cream 1L\", \"Fern Ghee 700g\",\n",
    "        \"Persil Gel 3L\", \"Pampers Size 4\", \"Dettol Liquid 500ml\"\n",
    "    ],\n",
    "\n",
    "    # أجهزة منزلية (High Ticket Items)\n",
    "    \"Home_Appliances\": [\n",
    "        \"Philips Air Fryer XL\", \"Black and Decker Espresso Machine\", \n",
    "        \"Tornado Electric Kettle 1.7L\", \"Fresh Microwave 25L\"\n",
    "    ],\n",
    "\n",
    "    # إكسسوارات براندات (High Quality Data)\n",
    "    \"Tech_Accessories\": [\n",
    "        \"AirPods Pro 2\", \"Samsung Buds FE\", \"Anker PowerCore 20000\",\n",
    "        \"Apple Watch Series 9\", \"Xiaomi Smart Band 8\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# ===================== Helpers =====================\n",
    "def clean_text(x: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (x or \"\")).strip()\n",
    "\n",
    "\n",
    "def is_blocked(html: str) -> bool:\n",
    "    return bool(re.search(\n",
    "        r\"Robot Check|Enter the characters you see|Sorry, we just need to make sure|captcha\",\n",
    "        html,\n",
    "        re.I\n",
    "    ))\n",
    "\n",
    "\n",
    "def build_driver(headless: bool = False):\n",
    "    options = uc.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "\n",
    "    options.add_argument(\"--window-size=1280,900\")\n",
    "    options.add_argument(\"--lang=en-US\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\n",
    "        \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    driver = uc.Chrome(options=options)\n",
    "    driver.set_page_load_timeout(60)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def human_scroll(driver):\n",
    "    steps = random.randint(2, 4)\n",
    "    for i in range(steps):\n",
    "        driver.execute_script(\n",
    "            f\"window.scrollTo(0, document.body.scrollHeight*{(i+1)/(steps+1)});\"\n",
    "        )\n",
    "        time.sleep(random.uniform(1.0, 2.0))\n",
    "\n",
    "\n",
    "def fetch_html(driver, url: str) -> str:\n",
    "    driver.get(url)\n",
    "    time.sleep(random.uniform(3.5, 5.5))\n",
    "    human_scroll(driver)\n",
    "    time.sleep(random.uniform(1.5, 2.5))\n",
    "    return driver.page_source\n",
    "\n",
    "\n",
    "def get_search_url(query: str, page: int) -> str:\n",
    "    return f\"{BASE}/s?k={quote_plus(query)}&page={page}\"\n",
    "\n",
    "\n",
    "# ===================== Extractors =====================\n",
    "def extract_title_and_url(card):\n",
    "    title, url = \"\", \"\"\n",
    "    h2 = card.select_one(\"h2.a-size-base-plus\")\n",
    "\n",
    "    if h2:\n",
    "        if h2.get(\"aria-label\"):\n",
    "            title = clean_text(h2.get(\"aria-label\"))\n",
    "        if not title:\n",
    "            span = h2.select_one(\"span\")\n",
    "            if span:\n",
    "                title = clean_text(span.get_text())\n",
    "\n",
    "        a = h2.find_parent(\"a\", href=True)\n",
    "        if a and a.get(\"href\"):\n",
    "            url = urljoin(BASE, a.get(\"href\"))\n",
    "\n",
    "    if not url:\n",
    "        a2 = card.select_one('a[href*=\"/dp/\"]')\n",
    "        if a2 and a2.get(\"href\"):\n",
    "            url = urljoin(BASE, a2.get(\"href\"))\n",
    "\n",
    "    return title, url\n",
    "\n",
    "\n",
    "def extract_price(card) -> str:\n",
    "    price_el = card.select_one(\"span.a-price > span.a-offscreen\")\n",
    "    return clean_text(price_el.get_text()) if price_el else \"\"\n",
    "\n",
    "\n",
    "def extract_old_price(card) -> str:\n",
    "    for prefix in (\"List:\", \"Was:\"):\n",
    "        block = card.select_one(\n",
    "            f'div[aria-hidden^=\"{prefix}\"] span.a-price.a-text-price span.a-offscreen'\n",
    "        )\n",
    "        if block and clean_text(block.get_text()):\n",
    "            return clean_text(block.get_text())\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_rating_reviews(card):\n",
    "    rating_el = card.select_one(\"span.a-icon-alt\")\n",
    "    rating = clean_text(rating_el.get_text()) if rating_el else \"\"\n",
    "\n",
    "    reviews_el = card.select_one('a[href*=\"#customerReviews\"] span') or card.select_one(\"span.a-size-base\")\n",
    "    reviews = clean_text(reviews_el.get_text()) if reviews_el else \"\"\n",
    "\n",
    "    return rating, reviews\n",
    "\n",
    "\n",
    "def extract_stock_status(card):\n",
    "    aria_span = card.select_one('span[aria-label*=\"stock\" i], span[aria-label*=\"unavailable\" i]')\n",
    "    aria_msg = clean_text(aria_span.get(\"aria-label\")) if aria_span and aria_span.get(\"aria-label\") else \"\"\n",
    "\n",
    "    if aria_msg:\n",
    "        msg_l = aria_msg.lower()\n",
    "        if \"left in stock\" in msg_l or \"in stock\" in msg_l:\n",
    "            return \"in_stock\", aria_msg\n",
    "        if \"currently unavailable\" in msg_l or \"out of stock\" in msg_l or \"unavailable\" in msg_l:\n",
    "            return \"out_of_stock\", aria_msg\n",
    "\n",
    "    full_text = card.get_text(\" \", strip=True).lower()\n",
    "    if \"currently unavailable\" in full_text or \"out of stock\" in full_text:\n",
    "        return \"out_of_stock\", \"Currently unavailable / Out of stock\"\n",
    "    if \"left in stock\" in full_text or \"in stock\" in full_text:\n",
    "        return \"in_stock\", \"In stock\"\n",
    "\n",
    "    return \"unknown\", \"\"\n",
    "\n",
    "\n",
    "# ===================== Parse page =====================\n",
    "def parse_search_page(html: str, category: str, query: str):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    rows = []\n",
    "\n",
    "    for card in soup.select('div[data-component-type=\"s-search-result\"][data-asin]'):\n",
    "        asin = (card.get(\"data-asin\") or \"\").strip()\n",
    "        if not asin:\n",
    "            continue\n",
    "\n",
    "        title, url = extract_title_and_url(card)\n",
    "        price = extract_price(card)\n",
    "        old_price = extract_old_price(card)\n",
    "        rating, reviews = extract_rating_reviews(card)\n",
    "        stock_status, stock_text = extract_stock_status(card)\n",
    "\n",
    "        if not title and not url:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"scrape_date\": SCRAPE_DATE,\n",
    "            \"category\": category,\n",
    "            \"query\": query,\n",
    "            \"asin\": asin,\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"price\": price,\n",
    "            \"old_price\": old_price,\n",
    "            \"rating\": rating,\n",
    "            \"reviews\": reviews,\n",
    "            \"stock_status\": stock_status,\n",
    "            \"stock_text\": stock_text\n",
    "        })\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ===================== Page count logic =====================\n",
    "def extract_total_results(html: str) -> int:\n",
    "    \"\"\"\n",
    "    Amazon غالبًا بيكتب:\n",
    "    \"1-48 of over 10,000 results for\"\n",
    "    أو \"1-48 of 312 results for\"\n",
    "    هنستخرج الرقم النهائي ونحوّله int.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    txt = \"\"\n",
    "\n",
    "    # أشكال شائعة لنتائج البحث\n",
    "    el = soup.select_one(\"span.sg-col-inner .a-section.a-spacing-small.a-spacing-top-small\")\n",
    "    if el:\n",
    "        txt = el.get_text(\" \", strip=True)\n",
    "    if not txt:\n",
    "        # fallback: search in whole page text (خفيف)\n",
    "        txt = soup.get_text(\" \", strip=True)\n",
    "\n",
    "    # نمط: of 312 results / of over 10,000 results\n",
    "    m = re.search(r\"of\\s+(over\\s+)?([\\d,]+)\\s+results\", txt, re.I)\n",
    "    if not m:\n",
    "        return 0\n",
    "\n",
    "    num = m.group(2).replace(\",\", \"\")\n",
    "    try:\n",
    "        return int(num)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def compute_pages_to_scrape(first_page_html: str, max_pages: int) -> int:\n",
    "    total_results = extract_total_results(first_page_html)\n",
    "    if total_results <= 0:\n",
    "        return 0\n",
    "\n",
    "    # Amazon عادة 48 نتيجة لكل صفحة على الديسكتوب (أحيانًا 24/16)\n",
    "    # هنفترض 48 ونستخدم ceil. حتى لو مختلف شويه، عندنا early stop لو صفحة فاضية.\n",
    "    estimated_pages = max(1, math.ceil(total_results / 48))\n",
    "    return min(estimated_pages, max_pages)\n",
    "\n",
    "\n",
    "# ===================== Main Scraper =====================\n",
    "def scrape_daily(search_map: dict, max_pages_per_query: int = MAX_PAGES_PER_QUERY, out_csv: str = OUT_CSV, headless: bool = False):\n",
    "    driver = build_driver(headless=headless)\n",
    "    all_rows = []\n",
    "\n",
    "    try:\n",
    "        # Warm session\n",
    "        driver.get(BASE)\n",
    "        time.sleep(random.uniform(2.0, 4.0))\n",
    "\n",
    "        for category, queries in search_map.items():\n",
    "            for q in queries:\n",
    "                # ---- Fetch page 1 first (to detect pages/results) ----\n",
    "                url1 = get_search_url(q, 1)\n",
    "                html1 = fetch_html(driver, url1)\n",
    "\n",
    "                if is_blocked(html1):\n",
    "                    raise RuntimeError(\"CAPTCHA / Robot Check ظهر. شغّلي headless=False وحليه مرة ثم أعيدي التشغيل.\")\n",
    "\n",
    "                pages_to_scrape = compute_pages_to_scrape(html1, max_pages_per_query)\n",
    "\n",
    "                if pages_to_scrape == 0:\n",
    "                    print(f\"[{SCRAPE_DATE}] SKIP (no results): Category='{category}' Query='{q}'\")\n",
    "                    continue\n",
    "\n",
    "                # ---- Parse page 1 ----\n",
    "                rows1 = parse_search_page(html1, category=category, query=q)\n",
    "                print(f\"[{SCRAPE_DATE}] Category='{category}' Query='{q}' Page=1/{pages_to_scrape} -> {len(rows1)} items\")\n",
    "                all_rows.extend(rows1)\n",
    "\n",
    "                # Early stop لو الصفحة الأولى مافيهاش منتجات فعلية\n",
    "                if len(rows1) == 0:\n",
    "                    continue\n",
    "\n",
    "                time.sleep(random.uniform(3.0, 6.0))\n",
    "\n",
    "                # ---- باقي الصفحات حتى الحد ----\n",
    "                for p in range(2, pages_to_scrape + 1):\n",
    "                    urlp = get_search_url(q, p)\n",
    "                    htmlp = fetch_html(driver, urlp)\n",
    "\n",
    "                    if is_blocked(htmlp):\n",
    "                        raise RuntimeError(\"CAPTCHA / Robot Check ظهر أثناء الصفحات. قللي عدد الصفحات/زودي delays.\")\n",
    "\n",
    "                    rowsp = parse_search_page(htmlp, category=category, query=q)\n",
    "                    print(f\"[{SCRAPE_DATE}] Category='{category}' Query='{q}' Page={p}/{pages_to_scrape} -> {len(rowsp)} items\")\n",
    "                    all_rows.extend(rowsp)\n",
    "\n",
    "                    # ✅ لو صفحة طلعت فاضية نوقف بدري\n",
    "                    if len(rowsp) == 0:\n",
    "                        break\n",
    "\n",
    "                    time.sleep(random.uniform(3.0, 6.0))\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    # Save CSV\n",
    "    if all_rows:\n",
    "        with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=list(all_rows[0].keys()))\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_rows)\n",
    "\n",
    "    return all_rows\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_daily(\n",
    "        search_map=SEARCH_MAP,\n",
    "        max_pages_per_query=MAX_PAGES_PER_QUERY,  # ✅ يجمع لحد 5\n",
    "        out_csv=OUT_CSV,\n",
    "        headless=False\n",
    "    )\n",
    "    print(\"DONE:\", len(data), \"->\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fabfcc-cbc9-4e44-a295-0346ac205224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9130c7-200e-446c-a40d-ccde033a85eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c6804a-a1b8-45ff-9db0-301fec5fa73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab441d3-5cd1-4e8e-b58f-30e85dcdf290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Electronics & Gadgets (Targeting high price fluctuations)\n",
    "electronics_keywords = [\n",
    "    \"iPhone 15\", \"iPhone 14\", \"Samsung Galaxy S23\", \"Samsung Galaxy A54\", \n",
    "    \"Xiaomi Redmi Note 12\", \"Realme 11\", \"MacBook Air M2\", \"HP Pavilion\", \n",
    "    \"Dell Vostro\", \"Lenovo IdeaPad\", \"ASUS Vivobook\", \"AirPods Pro\", \n",
    "    \"Samsung Buds\", \"Smart Watch Ultra\", \"Power Bank 20000mAh\", \n",
    "    \"Air Fryer\", \"Espresso Machine\", \"Microwave\", \"Electric Kettle\"\n",
    "]\n",
    "\n",
    "# 2. Groceries & Essentials (Targeting inflation tracking)\n",
    "grocery_keywords = [\n",
    "    \"Sunflower Oil 1L\", \"Corn Oil\", \"White Rice 1kg\", \"Pasta 400g\", \n",
    "    \"Sugar 1kg\", \"Ghee\", \"Full Cream Milk 1L\", \"Instant Coffee 200g\", \n",
    "    \"Tea Bags 100\", \"Cheddar Cheese\", \"Laundry Detergent Powder\", \n",
    "    \"Dishwashing Liquid\", \"Shampoo 400ml\", \"Toilet Paper\"\n",
    "]\n",
    "\n",
    "# 3. All Keywords (If you want to run one big loop)\n",
    "search_queries = electronics_keywords + grocery_keywords\n",
    "\n",
    "# Example of how to use it in your scraper:\n",
    "# for query in search_queries:\n",
    "#     print(f\"Scraping results for: {query}\")\n",
    "#     # Your scraping logic here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85caaf97-560f-4e3b-8241-e3a122ed9604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63edaa94-c269-4c11-9811-895bc3f26a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15dd97-c9e3-42fc-9ce7-1e942e07c1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790323c-36ec-4565-911a-3ceb28511b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e489b8-0dac-490a-9bac-bbbf8670ea11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c36d45-b43b-4407-9bca-450c49319b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "138b390c-fcc7-4588-b295-580c4f1e7e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE = \"https://www.amazon.eg\"\n",
    "\n",
    "def clean_text(x: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (x or \"\")).strip()\n",
    "\n",
    "def extract_title_and_url(card):\n",
    "    \"\"\"\n",
    "    Amazon search cards vary a lot, so we try multiple selectors.\n",
    "    Return: (title, url)\n",
    "    \"\"\"\n",
    "    # 1) Most common: h2 a\n",
    "    a = card.select_one(\"h2 a.a-link-normal[href]\")\n",
    "    if not a:\n",
    "        # 2) Sometimes anchor has different classes\n",
    "        a = card.select_one('a.a-link-normal.s-underline-text.s-underline-link-text.s-link-style[href]')\n",
    "    if not a:\n",
    "        # 3) Fallback: image container link often exists\n",
    "        a = card.select_one('span[data-component-type=\"s-product-image\"] a.a-link-normal[href]')\n",
    "    if not a:\n",
    "        # 4) Any product link that looks like /dp/...\n",
    "        a = card.select_one('a[href*=\"/dp/\"]')\n",
    "\n",
    "    url = urljoin(BASE, a.get(\"href\")) if a else \"\"\n",
    "\n",
    "    # Title could be inside h2 span OR in aria-label\n",
    "    title = \"\"\n",
    "    if a:\n",
    "        span = a.select_one(\"span\")\n",
    "        if span and clean_text(span.get_text()):\n",
    "            title = clean_text(span.get_text())\n",
    "        else:\n",
    "            # sometimes title is on h2 aria-label\n",
    "            h2 = card.select_one(\"h2\")\n",
    "            if h2 and h2.get(\"aria-label\"):\n",
    "                title = clean_text(h2.get(\"aria-label\"))\n",
    "\n",
    "    # Another fallback: h2 span directly\n",
    "    if not title:\n",
    "        t = card.select_one(\"h2 span\")\n",
    "        if t:\n",
    "            title = clean_text(t.get_text())\n",
    "\n",
    "    return title, url\n",
    "\n",
    "def extract_price(card):\n",
    "    # current price\n",
    "    price_el = card.select_one(\"span.a-price > span.a-offscreen\")\n",
    "    price = clean_text(price_el.get_text()) if price_el else \"\"\n",
    "\n",
    "    return price\n",
    "\n",
    "def extract_old_price(card):\n",
    "    \"\"\"\n",
    "    Old price appears as:\n",
    "    - List: ... OR Was: ... inside div[aria-hidden^=\"List:\"] / div[aria-hidden^=\"Was:\"]\n",
    "    We grab the numeric from span.a-price.a-text-price span.a-offscreen.\n",
    "    \"\"\"\n",
    "    old = \"\"\n",
    "\n",
    "    # Prefer \"List:\" then \"Was:\" (you can swap priority if you want)\n",
    "    for prefix in (\"List:\", \"Was:\"):\n",
    "        block = card.select_one(f'div[aria-hidden^=\"{prefix}\"] span.a-price.a-text-price span.a-offscreen')\n",
    "        if block and clean_text(block.get_text()):\n",
    "            old = clean_text(block.get_text())\n",
    "            break\n",
    "\n",
    "    return old\n",
    "\n",
    "def extract_rating_reviews(card):\n",
    "    rating_el = card.select_one(\"span.a-icon-alt\")  # e.g. \"4.1 out of 5 stars\"\n",
    "    rating = clean_text(rating_el.get_text()) if rating_el else \"\"\n",
    "\n",
    "    # reviews count commonly in: a[href*=\"#customerReviews\"] span, or span.a-size-base\n",
    "    reviews_el = card.select_one('a[href*=\"#customerReviews\"] span') or card.select_one(\"span.a-size-base\")\n",
    "    reviews = clean_text(reviews_el.get_text()) if reviews_el else \"\"\n",
    "\n",
    "    return rating, reviews\n",
    "\n",
    "def parse_search_page(html: str, query: str):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    rows = []\n",
    "    for card in soup.select('div[data-component-type=\"s-search-result\"][data-asin]'):\n",
    "        asin = (card.get(\"data-asin\") or \"\").strip()\n",
    "        if not asin:\n",
    "            continue\n",
    "\n",
    "        title, url = extract_title_and_url(card)\n",
    "        price = extract_price(card)\n",
    "        old_price = extract_old_price(card)\n",
    "        rating, reviews = extract_rating_reviews(card)\n",
    "\n",
    "        # Skip junk results without url/title\n",
    "        if not url and not title:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"query\": query,\n",
    "            \"asin\": asin,\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"price\": price,\n",
    "            \"old_price\": old_price,\n",
    "            \"rating\": rating,\n",
    "            \"reviews\": reviews,\n",
    "        })\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe5fde6-6e24-401c-a3d9-1c0ab09dcb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query='mobile' Page=1 -> 48 items\n",
      "Query='mobile' Page=2 -> 51 items\n",
      "Query='mobile' Page=3 -> 51 items\n",
      "Query='mobile' Page=4 -> 51 items\n",
      "Query='mobile' Page=5 -> 51 items\n",
      "Query='iphone' Page=1 -> 48 items\n",
      "Query='iphone' Page=2 -> 48 items\n",
      "Query='iphone' Page=3 -> 53 items\n",
      "Query='iphone' Page=4 -> 53 items\n",
      "Query='iphone' Page=5 -> 53 items\n",
      "Query='samsung' Page=1 -> 60 items\n",
      "Query='samsung' Page=2 -> 58 items\n",
      "Query='samsung' Page=3 -> 58 items\n",
      "Query='samsung' Page=4 -> 58 items\n",
      "Query='samsung' Page=5 -> 58 items\n",
      "Query='xiaomi redmi' Page=1 -> 50 items\n",
      "Query='xiaomi redmi' Page=2 -> 51 items\n",
      "Query='xiaomi redmi' Page=3 -> 51 items\n",
      "Query='xiaomi redmi' Page=4 -> 51 items\n",
      "Query='xiaomi redmi' Page=5 -> 50 items\n",
      "DONE 1052\n"
     ]
    }
   ],
   "source": [
    "import time, random, csv\n",
    "from urllib.parse import quote_plus\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "BASE = \"https://www.amazon.eg\"\n",
    "\n",
    "def selenium_html(url: str, headless=False) -> str:\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1280,900\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_argument(\"--lang=en-US\")\n",
    "    opts.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(4)\n",
    "\n",
    "        # help load cards\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight*0.7);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        return driver.page_source\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def scrape_queries(queries, pages_per_query=3, out_csv=\"amazon_eg_multi.csv\", headless=False):\n",
    "    all_rows = []\n",
    "\n",
    "    for q in queries:\n",
    "        for p in range(1, pages_per_query + 1):\n",
    "            url = f\"{BASE}/s?k={quote_plus(q)}&page={p}\"\n",
    "            html = selenium_html(url, headless=headless)\n",
    "\n",
    "            # IMPORTANT: uses parse_search_page from section (1)\n",
    "            rows = parse_search_page(html, query=q)\n",
    "\n",
    "            print(f\"Query='{q}' Page={p} -> {len(rows)} items\")\n",
    "            all_rows.extend(rows)\n",
    "\n",
    "            time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    if all_rows:\n",
    "        with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "            w = csv.DictWriter(f, fieldnames=list(all_rows[0].keys()))\n",
    "            w.writeheader()\n",
    "            w.writerows(all_rows)\n",
    "\n",
    "    return all_rows\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ITEMS_TO_SEARCH = [\"mobile\", \"iphone\", \"samsung\", \"xiaomi redmi\"]\n",
    "    data = scrape_queries(ITEMS_TO_SEARCH, pages_per_query=5, out_csv=\"amazon_eg_multi.csv\", headless=False)\n",
    "    print(\"DONE\", len(data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
